{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f113b2-be8d-4380-b761-9991dff0ddec",
   "metadata": {},
   "source": [
    "# Test Proteins Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de84484-acd3-421d-95ca-87c35aecba73",
   "metadata": {},
   "source": [
    "In this notebook, we will preprocess the data that will be used for testing a protein function prediction model, i.e. we will embed the test protein sequences into a vector format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4afe770-2aa4-419b-92d1-f7dfd79ba15b",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f525673-567f-4db2-a0a9-4127ceff9920",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175ee38-ac9b-4115-94a1-01ff4f7cfe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5EncoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec785e8-0f5f-4e65-9707-b3d80354875c",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa102e50-321a-4ef8-82a2-984e02a6b320",
   "metadata": {},
   "source": [
    "### Protein Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e7a5b-412d-4ad0-a8e5-ed54b150bd10",
   "metadata": {},
   "source": [
    "Our data is composed of protein sequences (a string of letters), where each one-letter or three-letter code represents an amino acid and corresponding taxonomyID. The sequences can be found in the file `testsuperset.fasta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905a90c6-2c7a-4a6f-9c13-1ce818d6282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = SeqIO.parse('./data/cafa-5-protein-function-prediction/Test (Targets)/testsuperset.fasta', \"fasta\")\n",
    "test_proteins = {}\n",
    "\n",
    "for protein in proteins:\n",
    "    taxonomyID = protein.description.split()[1]\n",
    "    test_proteins[protein.id] = {'sequence': str(protein.seq), 'taxonomyID': taxonomyID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9511f-169e-49fc-aa29-609502b3b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_proteins.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5421fff4-7c97-40ed-aa60-0e11a708b520",
   "metadata": {},
   "source": [
    "### Taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2542c50e-bb87-4f90-8422-e77b587a8cf9",
   "metadata": {},
   "source": [
    "The file `testsuperset-taxon-list.tsv` contains list of taxonomies description. The first columns is the taxon ID and the second is the corresponding description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06febdc-f789-4a96-9a81-d1aca4d8f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_taxonomy = pd.read_csv('./data/cafa-5-protein-function-prediction/Test (Targets)/testsuperset-taxon-list.tsv', sep='\\t', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d235d1b5-e894-4588-b41d-5962d10e90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_taxonomy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e419cc7-ea90-4b03-811d-3d1d8d2ce05f",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4011e4-8dee-49ef-a8f2-432c4f763709",
   "metadata": {},
   "source": [
    "We will start by initializing the tokenizer and the model, which we will use to generate the protein embeddings. We will be using an encoder-only, half-precision version of the ProtT5-XL-UniRef50 model, which is pretrained on a large corpus of protein sequences in a self-supervised fashion. This version will help us generate protein embeddings even with low GPU-memort, because it is fully usable on 8 GB of video RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272d775-e19b-4130-8fa6-8a4448c6fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1fd36-77ab-4008-a8f6-b468754abed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = list(test_proteins.keys())\n",
    "sequences = sorted(\n",
    "    [re.sub(r\"[UZOB]\", \"X\", protein['sequence']) for protein in test_proteins.values()],\n",
    "    key=len,\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for seq in sequences[:3]:\n",
    "    print(len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d72e0-5b1c-4617-ad21-8f09e7d57041",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = list()\n",
    "max_batch=100\n",
    "max_residues=4000\n",
    "max_seq_len=1000\n",
    "\n",
    "for idx, seq in enumerate(sequences):\n",
    "    seq_len = len(seq)\n",
    "    seq = ' '.join(list(seq))\n",
    "    batch.append((all_ids[idx], seq, seq_len))\n",
    "\n",
    "    n_res_batch = sum([s_len for _, _, s_len in batch]) + seq_len\n",
    "    if len(batch) >= max_batch or n_res_batch >= max_residues or idx == len(sequences) or seq_len > max_seq_len:\n",
    "        protein_ids, seqs, seq_lens = zip(*batch)\n",
    "        batch = list()\n",
    "\n",
    "        token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
    "        input_ids = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        except RuntimeError:\n",
    "            print(\"RuntimeError during embedding for {} (L={})\".format(protein_ids[idx], seq_len))\n",
    "            continue\n",
    "\n",
    "        for batch_idx, identifier in enumerate(protein_ids):\n",
    "            s_len = seq_lens[batch_idx]\n",
    "            protein_emb = embedding_repr.last_hidden_state[batch_idx,:s_len].mean(dim=0)\n",
    "\n",
    "            test_proteins[identifier]['embedding'] = protein_emb.detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c816423e-b71e-4d74-aea5-d8b3baa42cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_proteins.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e8ad6-1db6-4be4-b401-d2c6ede75e16",
   "metadata": {},
   "source": [
    "## Integrating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c6eb2-9569-41ca-89fb-09d53dd4cec5",
   "metadata": {},
   "source": [
    "Now when we have our protein sequences prepared, we can simply integrate the data, which will give us only one table, that contains all protein sequences and their corresponding taxonomy and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0d8e4a-6269-4ebd-9154-d8dad118448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_taxonomy = dict(zip(test_taxonomy['ID'], test_taxonomy['Species']))\n",
    "\n",
    "for protein in test_proteins.values():\n",
    "    if int(protein['taxonomyID']) in lookup_taxonomy:        \n",
    "        protein['taxonomy'] = lookup_taxonomy[int(protein['taxonomyID'])]\n",
    "    else:\n",
    "        protein['taxonomy'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782fd88a-119a-4a4f-9601-c780557b20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_proteins.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41c66d0-c2d5-47d0-99f1-d85e22af04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "features = []\n",
    "\n",
    "for protein_id, data in test_proteins.items():\n",
    "    data_list.append([\n",
    "        protein_id,\n",
    "        data['taxonomyID'],\n",
    "        data['taxonomy'],\n",
    "        data['sequence'],\n",
    "        data['embedding']\n",
    "    ])\n",
    "\n",
    "    protein_features = [\n",
    "        protein_id,\n",
    "        data['taxonomyID'],\n",
    "    ]\n",
    "\n",
    "    protein_features.extend(data['embedding'])\n",
    "    features.append(protein_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02133b5b-8a6f-462f-81d3-5cc5d9ff33f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(data_list, columns=['ProteinID', 'TaxonomyID', 'Taxonomy', 'Sequence', 'Embedding'])\n",
    "test_df.set_index('ProteinID', inplace=True)\n",
    "test_df.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e5d2b-5fda-4c89-ac32-d9bdc9de88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a484f-07eb-4513-a7de-326769f4c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['ProteinID', 'TaxonomyID'] + ['Embed_' + str(i+1) for i in range(1024)]\n",
    "X_test = pd.DataFrame(features, columns=column_names)\n",
    "X_test.set_index('ProteinID', inplace=True)\n",
    "X_test.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880ff0b-6162-42e2-b7c8-2311b7ea487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc4eb13-f99e-41c6-8635-8bd2e280a900",
   "metadata": {},
   "source": [
    "## Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade789c-2e1e-4028-8efc-ef1a8168b1de",
   "metadata": {},
   "source": [
    "Let's save the embedded data as CSV files for testing so we could easily access them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f91f2f-0fce-4ac9-a87c-6dbf6b76afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('data_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad71ae-b8b0-4434-a7c3-926990bdc9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('features_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
