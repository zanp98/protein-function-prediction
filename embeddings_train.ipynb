{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36439407-b7df-4ca0-b538-e4a2d8052228",
   "metadata": {
    "id": "36439407-b7df-4ca0-b538-e4a2d8052228"
   },
   "source": [
    "# Training Protein Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128ee4b3-462f-46ae-bd49-d160a969f19b",
   "metadata": {
    "id": "128ee4b3-462f-46ae-bd49-d160a969f19b"
   },
   "source": [
    "In this notebook, we will preprocess the data that will be used for training a protein function prediction model, i.e. we will embed the training protein sequences into a vector format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc38c9-0896-4c9f-b27d-0bf3b1c608bc",
   "metadata": {
    "id": "c8dc38c9-0896-4c9f-b27d-0bf3b1c608bc"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78195400-d02e-447c-aa63-caae5909c6eb",
   "metadata": {
    "id": "78195400-d02e-447c-aa63-caae5909c6eb"
   },
   "outputs": [],
   "source": [
    "!pip install Bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b094c9-2f6b-4125-9c2b-0f3fc065ee65",
   "metadata": {
    "id": "11b094c9-2f6b-4125-9c2b-0f3fc065ee65"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5EncoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47379bbf-2b5d-40ac-83cf-bba4a2137643",
   "metadata": {
    "id": "47379bbf-2b5d-40ac-83cf-bba4a2137643"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a41995-f931-4a5a-90fe-be3132c78c56",
   "metadata": {
    "id": "52a41995-f931-4a5a-90fe-be3132c78c56"
   },
   "source": [
    "The objective of the model will be to predict the terms (functions) of a protein sequence. It's important to keep in mind that one protein sequence can have many functions (GO Term IDs) and all of them must be predicted by our model for each protein sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c1dad2-7af2-42b1-b626-1103fb5ff9f1",
   "metadata": {
    "id": "a2c1dad2-7af2-42b1-b626-1103fb5ff9f1"
   },
   "source": [
    "### Protein Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf03c2e-a1fd-45db-94e0-b66693fe6f98",
   "metadata": {
    "id": "4cf03c2e-a1fd-45db-94e0-b66693fe6f98"
   },
   "source": [
    "Our data is composed of protein sequences (a string of letters), where each one-letter or three-letter code represents an amino acid. The sequences can be found in the file `train_sequences.fasta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca610b2-ab2e-4423-858e-3dc61ab23656",
   "metadata": {
    "id": "cca610b2-ab2e-4423-858e-3dc61ab23656"
   },
   "outputs": [],
   "source": [
    "proteins = SeqIO.parse('./data/cafa-5-protein-function-prediction/Train/train_sequences.fasta', \"fasta\")\n",
    "train_proteins = {}\n",
    "\n",
    "for protein in proteins:\n",
    "    train_proteins[protein.id] = {'sequence': str(protein.seq), 'GO': {'BPO': [], 'CCO': [], 'MFO': []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd22621-cd69-4933-8fc2-935214c468b7",
   "metadata": {
    "id": "6dd22621-cd69-4933-8fc2-935214c468b7"
   },
   "outputs": [],
   "source": [
    "list(train_proteins.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf93885-155f-4c54-b217-9aa954beb9aa",
   "metadata": {
    "id": "8cf93885-155f-4c54-b217-9aa954beb9aa"
   },
   "source": [
    "### Taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700bed7d-1c09-4570-bd1c-b83e115ada40",
   "metadata": {
    "id": "700bed7d-1c09-4570-bd1c-b83e115ada40"
   },
   "source": [
    "The file `train_taxonomy.tsv` contains list of proteins and the species to whuch they belong (taxonomy ID). The first columns is the protein UniProt accession ID and the second is the taxon ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffc2ba-87ca-4119-9231-c6211a1268ed",
   "metadata": {
    "id": "bbffc2ba-87ca-4119-9231-c6211a1268ed"
   },
   "outputs": [],
   "source": [
    "train_taxonomy = pd.read_csv('./data/cafa-5-protein-function-prediction/Train/train_taxonomy.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb3cc9d-ea9a-48ed-98ab-9eb1febcd916",
   "metadata": {
    "id": "1bb3cc9d-ea9a-48ed-98ab-9eb1febcd916"
   },
   "outputs": [],
   "source": [
    "train_taxonomy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f5991-5c13-42e6-8c3a-94c16f88150f",
   "metadata": {
    "id": "d83f5991-5c13-42e6-8c3a-94c16f88150f"
   },
   "source": [
    "### Gene Ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669a206-2a88-49da-ab19-efe148b1bd17",
   "metadata": {
    "id": "8669a206-2a88-49da-ab19-efe148b1bd17"
   },
   "source": [
    "The functional properties of proteins are defined using Gene Ontology (GO) with respect to Molecular Function (MF), Biological Process (BP), and Cellular Component (CC). List of annotated terms (ground thruths) for the protein sequences are available in the file `train_terms.fasta`, where the first column (attribute) is the protein's UniProt accession ID (unique protein id), the second is the GO Term ID, and the third is the ontology, in which the term appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58277fc3-ba8a-4c94-af88-5f2d9a2d5d2f",
   "metadata": {
    "id": "58277fc3-ba8a-4c94-af88-5f2d9a2d5d2f"
   },
   "outputs": [],
   "source": [
    "train_terms = pd.read_csv('./data/cafa-5-protein-function-prediction/Train/train_terms.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ec26a-0a80-40ef-9b18-47401dcd01b5",
   "metadata": {
    "id": "512ec26a-0a80-40ef-9b18-47401dcd01b5"
   },
   "outputs": [],
   "source": [
    "train_terms.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1daa1-511a-4f42-88ff-c9d5fe5573db",
   "metadata": {
    "id": "61d1daa1-511a-4f42-88ff-c9d5fe5573db"
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1454610-71f5-4cd9-a2d9-058a4a946da8",
   "metadata": {
    "id": "e1454610-71f5-4cd9-a2d9-058a4a946da8"
   },
   "source": [
    "We will start by initializing the tokenizer and the model, which we will use to generate the protein embeddings. We will be using an encoder-only, half-precision version of the [ProtT5-XL-UniRef50](https://huggingface.co/Rostlab/prot_t5_xl_uniref50) model, which is pretrained on a large corpus of protein sequences in a self-supervised fashion. [This version](https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc) will help us generate protein embeddings even with low GPU-memort, because it is fully usable on 8 GB of video RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f0bc33-5b99-4887-8824-8fcfc83f3b7e",
   "metadata": {
    "id": "84f0bc33-5b99-4887-8824-8fcfc83f3b7e"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UV-XdletC6JK",
   "metadata": {
    "id": "UV-XdletC6JK"
   },
   "outputs": [],
   "source": [
    "all_ids = list(train_proteins.keys())\n",
    "sequences = sorted(\n",
    "    [re.sub(r\"[UZOB]\", \"X\", protein['sequence']) for protein in train_proteins.values()],\n",
    "    key=len,\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for seq in sequences[:3]:\n",
    "    print(len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vqeeKUAQJDdH",
   "metadata": {
    "id": "vqeeKUAQJDdH"
   },
   "outputs": [],
   "source": [
    "batch = list()\n",
    "max_batch=100\n",
    "max_residues=4000\n",
    "max_seq_len=1000\n",
    "\n",
    "for idx, seq in enumerate(sequences):\n",
    "    seq_len = len(seq)\n",
    "    seq = ' '.join(list(seq))\n",
    "    batch.append((all_ids[idx], seq, seq_len))\n",
    "\n",
    "    n_res_batch = sum([s_len for _, _, s_len in batch]) + seq_len\n",
    "    if len(batch) >= max_batch or n_res_batch >= max_residues or idx == len(sequences) or seq_len > max_seq_len:\n",
    "        protein_ids, seqs, seq_lens = zip(*batch)\n",
    "        batch = list()\n",
    "\n",
    "        token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
    "        input_ids = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        except RuntimeError:\n",
    "            print(\"RuntimeError during embedding for {} (L={})\".format(protein_ids[idx], seq_len))\n",
    "            continue\n",
    "\n",
    "        for batch_idx, identifier in enumerate(protein_ids):\n",
    "            s_len = seq_lens[batch_idx]\n",
    "            protein_emb = embedding_repr.last_hidden_state[batch_idx,:s_len].mean(dim=0)\n",
    "\n",
    "            train_proteins[identifier]['embedding'] = protein_emb.detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e7f31-d05a-414b-9650-5e4dc66e3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_proteins.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60207af-09a8-43e3-ad95-434c9a5fd301",
   "metadata": {
    "id": "e60207af-09a8-43e3-ad95-434c9a5fd301"
   },
   "source": [
    "## Integrating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f2bbb2-035b-4d0a-beb4-a2722a73faaa",
   "metadata": {
    "id": "92f2bbb2-035b-4d0a-beb4-a2722a73faaa"
   },
   "source": [
    "Now when we have our protein sequences prepared, we can simply integrate the data, which will give us only one table, that contains all protein sequences and their corresponding taxonomy and GO Term IDs with respect to all aspects (MF, BP, and CC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457e20ff-ca7b-4829-b08b-d225ae919900",
   "metadata": {
    "id": "457e20ff-ca7b-4829-b08b-d225ae919900"
   },
   "outputs": [],
   "source": [
    "for _, row in train_taxonomy.iterrows():\n",
    "    if row['EntryID'] in train_proteins:\n",
    "        train_proteins[row['EntryID']]['taxonomyID'] = row['taxonomyID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a6028-e16a-416a-92ba-c90584d61ceb",
   "metadata": {
    "id": "2e0a6028-e16a-416a-92ba-c90584d61ceb"
   },
   "outputs": [],
   "source": [
    "for _, row in train_terms.iterrows():\n",
    "    if row['EntryID'] in train_proteins:\n",
    "        train_proteins[row['EntryID']]['GO'][row['aspect']].append(row['term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd557f04-f0cd-46c4-946e-09d876071b46",
   "metadata": {
    "id": "fd557f04-f0cd-46c4-946e-09d876071b46"
   },
   "outputs": [],
   "source": [
    "data_list = []\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for protein_id, data in train_proteins.items():\n",
    "    data_list.append([\n",
    "        protein_id,\n",
    "        data['taxonomyID'],\n",
    "        data['sequence'],\n",
    "        data['embedding'],\n",
    "        data['GO']['BPO'],\n",
    "        data['GO']['CCO'],\n",
    "        data['GO']['MFO']\n",
    "    ])\n",
    "\n",
    "    protein_features = [\n",
    "        protein_id,\n",
    "        data['taxonomyID'],\n",
    "    ]\n",
    "\n",
    "    protein_features.extend(data['embedding'])\n",
    "    features.append(protein_features)\n",
    "\n",
    "    labels.append([\n",
    "        protein_id,\n",
    "        data['GO']['BPO'],\n",
    "        data['GO']['CCO'],\n",
    "        data['GO']['MFO']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28446df-093d-4310-b16d-5c2ec09db60d",
   "metadata": {
    "id": "d28446df-093d-4310-b16d-5c2ec09db60d"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(data_list, columns=['ProteinID', 'TaxonomyID', 'Sequence', 'Embedding', 'BPO', 'CCO', 'MFO'])\n",
    "train_df.set_index('ProteinID', inplace=True)\n",
    "train_df.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95729eb0-d78e-4c29-ab76-de397e5091b6",
   "metadata": {
    "id": "95729eb0-d78e-4c29-ab76-de397e5091b6"
   },
   "outputs": [],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b72e230-8c90-4ce1-a7d7-f6fea42c2a84",
   "metadata": {
    "id": "6b72e230-8c90-4ce1-a7d7-f6fea42c2a84"
   },
   "outputs": [],
   "source": [
    "column_names = ['ProteinID', 'TaxonomyID'] + ['Embed_' + str(i+1) for i in range(1024)]\n",
    "X_train = pd.DataFrame(features, columns=column_names)\n",
    "X_train.set_index('ProteinID', inplace=True)\n",
    "X_train.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6838a-f6f4-4e9b-b74d-a665769ba3da",
   "metadata": {
    "id": "dac6838a-f6f4-4e9b-b74d-a665769ba3da"
   },
   "outputs": [],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0782fa7-1477-47ed-9e42-da48f0fa97f4",
   "metadata": {
    "id": "d0782fa7-1477-47ed-9e42-da48f0fa97f4"
   },
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(labels, columns=['ProteinID', 'BPO', 'CCO', 'MFO'])\n",
    "y_train.set_index('ProteinID', inplace=True)\n",
    "y_train.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48835f94-31e6-4feb-b8a2-17d3e100c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8d0d0-3673-4fe7-9b07-f018a477c989",
   "metadata": {
    "id": "f4a8d0d0-3673-4fe7-9b07-f018a477c989"
   },
   "source": [
    "## Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc06c8-55fa-4f8e-87ba-a8bc4fe375aa",
   "metadata": {
    "id": "cbcc06c8-55fa-4f8e-87ba-a8bc4fe375aa"
   },
   "source": [
    "Let's save the embedded data as CSV files for both training so we could easily access them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e144416-b635-48c0-badf-464e88fc7ef8",
   "metadata": {
    "id": "4e144416-b635-48c0-badf-464e88fc7ef8"
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('data_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f8186-ad96-4390-a289-cee6afd5bfa5",
   "metadata": {
    "id": "f72f8186-ad96-4390-a289-cee6afd5bfa5"
   },
   "outputs": [],
   "source": [
    "X_train.to_csv('features_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a823d71-1920-4f30-8687-b3321a806486",
   "metadata": {
    "id": "1a823d71-1920-4f30-8687-b3321a806486"
   },
   "outputs": [],
   "source": [
    "y_train.to_csv('labels_train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
