{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36439407-b7df-4ca0-b538-e4a2d8052228",
   "metadata": {},
   "source": [
    "# Training Protein Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128ee4b3-462f-46ae-bd49-d160a969f19b",
   "metadata": {},
   "source": [
    "In this notebook, we will preprocess the data that will be used for training a protein function prediction model, i.e. we will embed the training protein sequences into a vector format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc38c9-0896-4c9f-b27d-0bf3b1c608bc",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78195400-d02e-447c-aa63-caae5909c6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Bio in c:\\users\\nikol\\appdata\\roaming\\python\\python38\\site-packages (1.6.2)\n",
      "Requirement already satisfied: biopython>=1.80 in c:\\users\\nikol\\appdata\\roaming\\python\\python38\\site-packages (from Bio) (1.83)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from Bio) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from Bio) (4.66.2)\n",
      "Requirement already satisfied: mygene in c:\\users\\nikol\\appdata\\roaming\\python\\python38\\site-packages (from Bio) (3.2.2)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from Bio) (2.0.3)\n",
      "Requirement already satisfied: pooch in c:\\users\\nikol\\appdata\\roaming\\python\\python38\\site-packages (from Bio) (1.8.1)\n",
      "Requirement already satisfied: gprofiler-official in c:\\users\\nikol\\appdata\\roaming\\python\\python38\\site-packages (from Bio) (1.0.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from biopython>=1.80->Bio) (1.24.3)\n",
      "Requirement already satisfied: biothings-client>=0.2.6 in c:\\users\\nikol\\appdata\\roaming\\python\\python38\\site-packages (from mygene->Bio) (0.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from pandas->Bio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from pandas->Bio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from pandas->Bio) (2023.3)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from pooch->Bio) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from pooch->Bio) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from requests->Bio) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from requests->Bio) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from requests->Bio) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from requests->Bio) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from tqdm->Bio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\cuda_test\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->Bio) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b094c9-2f6b-4125-9c2b-0f3fc065ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5EncoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47379bbf-2b5d-40ac-83cf-bba4a2137643",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a41995-f931-4a5a-90fe-be3132c78c56",
   "metadata": {},
   "source": [
    "The objective of the model will be to predict the terms (functions) of a protein sequence. It's important to keep in mind that one protein sequence can have many functions (GO Term IDs) and all of them must be predicted by our model for each protein sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c1dad2-7af2-42b1-b626-1103fb5ff9f1",
   "metadata": {},
   "source": [
    "### Protein Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf03c2e-a1fd-45db-94e0-b66693fe6f98",
   "metadata": {},
   "source": [
    "Our data is composed of protein sequences (a string of letters), where each one-letter or three-letter code represents an amino acid. The sequences can be found in the file `train_sequences.fasta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca610b2-ab2e-4423-858e-3dc61ab23656",
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = SeqIO.parse('./data/cafa-5-protein-function-prediction/Train/train_sequences.fasta', \"fasta\")\n",
    "train_proteins = {}\n",
    "\n",
    "for protein in proteins:\n",
    "    train_proteins[protein.id] = {'sequence': str(protein.seq), 'GO': {'BPO': [], 'CCO': [], 'MFO': []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd22621-cd69-4933-8fc2-935214c468b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('P20536',\n",
       "  {'sequence': 'MNSVTVSHAPYTITYHDDWEPVMSQLVEFYNEVASWLLRDETSPIPDKFFIQLKQPLRNKRVCVCGIDPYPKDGTGVPFESPNFTKKSIKEIASSISRLTGVIDYKGYNLNIIDGVIPWNYYLSCKLGETKSHAIYWDKISKLLLQHITKHVSVLYCLGKTDFSNIRAKLESPVTTIVGYHPAARDRQFEKDRSFEIINVLLELDNKVPINWAQGFIY',\n",
       "   'GO': {'BPO': [], 'CCO': [], 'MFO': []}}),\n",
       " ('O73864',\n",
       "  {'sequence': 'MTEYRNFLLLFITSLSVIYPCTGISWLGLTINGSSVGWNQTHHCKLLDGLVPDQQQLCKRNLELMHSIVRAARLTKSACTSSFSDMRWNWSSIESAPHFTPDLAKGTREAAFVVSLAAAVVSHAIARACASGDLPSCSCAAMPSEQAAPDFRWGGCGDNLRYYGLQMGSAFSDAPMRNRRSGPQDFRLMQLHNNAVGRQVLMDSLEMKCKCHGVSGSCSVKTCWKGLQDISTISADLKSKYLSATKVIPRQIGTRRQLVPREMEVRPVGENELVYLVSSPDYCTQNAKQGSLGTTDRQCNKTASGSESCGLMCCGRGYNAYTEVLVERCQCKYHWCCYVSCKTCKRTVERYVSK',\n",
       "   'GO': {'BPO': [], 'CCO': [], 'MFO': []}}),\n",
       " ('O95231',\n",
       "  {'sequence': 'MRLSSSPPRGPQQLSSFGSVDWLSQSSCSGPTHTPRPADFSLGSLPGPGQTSGAREPPQAVSIKEAAGSSNLPAPERTMAGLSKEPNTLRAPRVRTAFTMEQVRTLEGVFQHHQYLSPLERKRLAREMQLSEVQIKTWFQNRRMKHKRQMQDPQLHSPFSGSLHAPPAFYSTSSGLANGLQLLCPWAPLSGPQALMLPPGSFWGLCQVAQEALASAGASCCGQPLASHPPTPGRPSLGPALSTGPRGLCAMPQTGDAF',\n",
       "   'GO': {'BPO': [], 'CCO': [], 'MFO': []}})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_proteins.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf93885-155f-4c54-b217-9aa954beb9aa",
   "metadata": {},
   "source": [
    "### Taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700bed7d-1c09-4570-bd1c-b83e115ada40",
   "metadata": {},
   "source": [
    "The file `train_taxonomy.tsv` contains list of proteins and the species to whuch they belong (taxonomy ID). The first columns is the protein UniProt accession ID and the second is the taxon ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbffc2ba-87ca-4119-9231-c6211a1268ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_taxonomy = pd.read_csv('./data/cafa-5-protein-function-prediction/Train/train_taxonomy.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb3cc9d-ea9a-48ed-98ab-9eb1febcd916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EntryID</th>\n",
       "      <th>taxonomyID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q8IXT2</td>\n",
       "      <td>9606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q04418</td>\n",
       "      <td>559292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A8DYA3</td>\n",
       "      <td>7227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  EntryID  taxonomyID\n",
       "0  Q8IXT2        9606\n",
       "1  Q04418      559292\n",
       "2  A8DYA3        7227"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_taxonomy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f5991-5c13-42e6-8c3a-94c16f88150f",
   "metadata": {},
   "source": [
    "### Gene Ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669a206-2a88-49da-ab19-efe148b1bd17",
   "metadata": {},
   "source": [
    "The functional properties of proteins are defined using Gene Ontology (GO) with respect to Molecular Function (MF), Biological Process (BP), and Cellular Component (CC). List of annotated terms (ground thruths) for the protein sequences are available in the file `train_terms.fasta`, where the first column (attribute) is the protein's UniProt accession ID (unique protein id), the second is the GO Term ID, and the third is the ontology, in which the term appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58277fc3-ba8a-4c94-af88-5f2d9a2d5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_terms = pd.read_csv('./data/cafa-5-protein-function-prediction/Train/train_terms.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "512ec26a-0a80-40ef-9b18-47401dcd01b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EntryID</th>\n",
       "      <th>term</th>\n",
       "      <th>aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A009IHW8</td>\n",
       "      <td>GO:0008152</td>\n",
       "      <td>BPO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A009IHW8</td>\n",
       "      <td>GO:0034655</td>\n",
       "      <td>BPO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A009IHW8</td>\n",
       "      <td>GO:0072523</td>\n",
       "      <td>BPO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      EntryID        term aspect\n",
       "0  A0A009IHW8  GO:0008152    BPO\n",
       "1  A0A009IHW8  GO:0034655    BPO\n",
       "2  A0A009IHW8  GO:0072523    BPO"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_terms.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1daa1-511a-4f42-88ff-c9d5fe5573db",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1454610-71f5-4cd9-a2d9-058a4a946da8",
   "metadata": {},
   "source": [
    "We will start by initializing the tokenizer and the model, which we will use to generate the protein embeddings. We will be using an encoder-only, half-precision version of the [ProtT5-XL-UniRef50](https://huggingface.co/Rostlab/prot_t5_xl_uniref50) model, which is pretrained on a large corpus of protein sequences in a self-supervised fashion. [This version](https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc) will help us generate protein embeddings even with low GPU-memort, because it is fully usable on 8 GB of video RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84f0bc33-5b99-4887-8824-8fcfc83f3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\", torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4920749-015c-4fa0-b442-f2adf3ed5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(sequence):\n",
    "    # Map the rare amino acids \"U,Z,O,B\" to \"X\"\n",
    "    seq = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence)))]\n",
    "\n",
    "    # Encode the sequence\n",
    "    ids = tokenizer.batch_encode_plus(seq, add_special_tokens=True, padding=\"longest\")\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "    # Generate the embedding\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    return output.last_hidden_state[0].mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177add0f-e524-48aa-aacc-3c22863b8a4e",
   "metadata": {},
   "source": [
    "Let's now convert the alphabetical protein sequences into a vector format (embeddings) that will be used to train our model. Since we talk about a sequence of letters, we can use a similar approach, used to generate word embeddings for an NLP model's training. Thus we can use any publicly available pre-trained protein embedding models to generate the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "941ce809-e0ed-45c8-8afb-2cb354485101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|                                                                | 0/142246 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNSVTVSHAPYTITYHDDWEPVMSQLVEFYNEVASWLLRDETSPIPDKFFIQLKQPLRNKRVCVCGIDPYPKDGTGVPFESPNFTKKSIKEIASSISRLTGVIDYKGYNLNIIDGVIPWNYYLSCKLGETKSHAIYWDKISKLLLQHITKHVSVLYCLGKTDFSNIRAKLESPVTTIVGYHPAARDRQFEKDRSFEIINVLLELDNKVPINWAQGFIY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|                                                      | 2/142246 [00:00<7:03:59,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTEYRNFLLLFITSLSVIYPCTGISWLGLTINGSSVGWNQTHHCKLLDGLVPDQQQLCKRNLELMHSIVRAARLTKSACTSSFSDMRWNWSSIESAPHFTPDLAKGTREAAFVVSLAAAVVSHAIARACASGDLPSCSCAAMPSEQAAPDFRWGGCGDNLRYYGLQMGSAFSDAPMRNRRSGPQDFRLMQLHNNAVGRQVLMDSLEMKCKCHGVSGSCSVKTCWKGLQDISTISADLKSKYLSATKVIPRQIGTRRQLVPREMEVRPVGENELVYLVSSPDYCTQNAKQGSLGTTDRQCNKTASGSESCGLMCCGRGYNAYTEVLVERCQCKYHWCCYVSCKTCKRTVERYVSK\n",
      "MRLSSSPPRGPQQLSSFGSVDWLSQSSCSGPTHTPRPADFSLGSLPGPGQTSGAREPPQAVSIKEAAGSSNLPAPERTMAGLSKEPNTLRAPRVRTAFTMEQVRTLEGVFQHHQYLSPLERKRLAREMQLSEVQIKTWFQNRRMKHKRQMQDPQLHSPFSGSLHAPPAFYSTSSGLANGLQLLCPWAPLSGPQALMLPPGSFWGLCQVAQEALASAGASCCGQPLASHPPTPGRPSLGPALSTGPRGLCAMPQTGDAF\n",
      "MGGEAGADGPRGRVKSLGLVFEDESKGCYSSGETVAGHVLLEAAEPVALRGLRLEAQGRATSAWGPSAGARVCIGGGSPAASSEVEYLNLRLSLLEAPAGEGVTLLQPGKHEFPFRFQLPSEPLATSFTGKYGSIQYCVRAVLERPQVPDQSVRRELQVVSHVDVNTPPLLTPMLKTQEKMVGCWLFTSGPVSLSVKIERKGYCNGEAIPIYAEIENCSSRLVVPKAAIFQTQTYLASGKTKTVRHMVANVRGNHIGSGSTDTWNGKMLKIPPVTPSILDCCIIRVDYSLAVYIHIPGAKRLMLELPLVIGTIPYSGFGRRNSSVASQFSMDMCWLALALPEQPEAPPNYADVVSEEEFSRHVPPYPQPSDCDGEACYSMFACIQEFRFQPPPLYSEVDPHPGDAQETQPVSFIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|                                                      | 5/142246 [00:00<4:53:37,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVETNSPPAGYTLKRSPSDLGEQQQPPRQISRSPGNTAAYHLTTAMLLNSQQCGYLGQRLQSVLQQQHAQHQQSQSQTPSSDDGSQSGVTILEEERRGGAAAASLFTIDSILGSRQQGGGTAPSQGSHISSNGNQNGLTSNGISLGLKRSGAESPASPNSNSSSSAAASPIRPQRVPAMLQHPGLHLGHLAAAAASGFAASPSDFLVAYPNFYPNYMHAAAVAHVAAAQMQAHVSGAAAGLSGHGHHPHHPHGHPHHPHLGAHHHGQHHLSHLGHGPPPKRKRRHRTIFTEEQLEQLEATFDKTHYPDVVLREQLALKVDLKEERVEVWFKNRRAKWRKQKREEQERLRKLQEEQCGSTTNGTTNSSSGTTSSTGNGSLTVKCPGSDHYSAQLVHIKSDANGYSDADESSDLEVA\n",
      "MGHTRRQGTSPSKCPYLNFFQLLVLAGLSHFCSGVIHVTKEVKEVATLSCGHNVSVEELAQTRIYWQKEKKMVLTMMSGDMNIWPEYKNRTIFDITNNLSIVILALRPSDEGTYECVVLKYEKDAFKREHLAEVTLSVKADFPTPSISDFEIPTSNIRRIICSTSGGFPEPHLSWLENGEELNAINTTVSQDPETELYAVSSKLDFNMTTNHSFMCLIKYGHLRVNQTFNWNTTKQEHFPDNLLPSWAITLISVNGIFVICCLTYCFAPRCRERRRNERLRRESVRPV\n",
      "MTIEKIFTPQDDAFYAVITHAAGPQGALPLTPQMLMESPSGNLFGMTQNAGMGWDANKLTGKEVLIIGTQGGIRAGDGRPIALGYHTGHWEIGMQMQAAAKEITRNGGIPFAAFVSDPCDGRSQGTHGMFDSLPYRNDAAIVFRRLIRSLPTRRAVIGVATCDKGLPATMIALAAMHDLPTILVPGGATLPPTVGEDAGKVQTIGARFANHELSLQEAAELGCRACASPGGGCQFLGTAGTSQVVAEALGLALPHSALAPSGQAVWLEIARQSARAVSELDSRGITTRDILSDKAIENAMVIHAAFGGSTNLLLHIPAIAHAAGCTIPDVEHWTRINRKVPRLVSVLPNGPDYHPTVRAFLAGGVPEVMLHLRDLGLLHLDAMTVTGQTVGENLEWWQASERRARFRQCLREQDGVEPDDVILPPEKAKAKGLTSTVCFPTGNIAPEGSVIKATAIDPSVVGEDGVYHHTGRVRVFVSEAQAIKAIKREEIVQGDIMVVIGGGPSGTGMEETYQLTSALKHISWGKTVSLITDARFSGVSTGACFGHVSPEALAGGPIGKLRDNDIIEIAVDRLTLTGSVNFIGTADNPLTPEEGARELARRQTHPDLHAHDFLPDDTRLWAALQSVSGGTWKGCIYDTDKIIEVINAGKKALGI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|                                                     | 7/142246 [00:01<11:27:02,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAAAARPRGRALGPVLPPTPLLLLVLRVLPACGATARDPGAAAGLSLHPTYFNLAEAARIWATATCGERGPGEGRPQPELYCKLVGGPTAPGSGHTIQGQFCDYCNSEDPRKAHPVTNAIDGSERWWQSPPLSSGTQYNRVNLTLDLGQLFHVAYILIKFANSPRPDLWVLERSVDFGSTYSPWQYFAHSKVDCLKEFGREANMAVTRDDDVLCVTEYSRIVPLENGEVVVSLINGRPGAKNFTFSHTLREFTKATNIRLRFLRTNTLLGHLISKAQRDPTVTRRYYYSIKDISIGGQCVCNGHAEVCNINNPEKLFRCECQHHTCGETCDRCCTGYNQRRWRPAAWEQSHECEACNCHGHASNCYYDPDVERQQASLNTQGIYAGGGVCINCQHNTAGVNCEQCAKGYYRPYGVPVDAPDGCIPCSCDPEHADGCEQGSGRCHCKPNFHGDNCEKCAIGYYNFPFCLRIPIFPVSTPSSEDPVAGDIKGCDCNLEGVLPEICDAHGRCLCRPGVEGPRCDTCRSGFYSFPICQACWCSALGSYQMPCSSVTGQCECRPGVTGQRCDRCLSGAYDFPHCQGSSSACDPAGTINSNLGYCQCKLHVEGPTCSRCKLLYWNLDKENPSGCSECKCHKAGTVSGTGECRQGDGDCHCKSHVGGDSCDTCEDGYFALEKSNYFGCQGCQCDIGGALSSMCSGPSGVCQCREHVVGKVCQRPENNYYFPDLHHMKYEIEDGSTPNGRDLRFGFDPLAFPEFSWRGYAQMTSVQNDVRITLNVGKSSGSLFRVILRYVNPGTEAVSGHITIYPSWGAAQSKEIIFLPSKEPAFVTVPGNGFADPFSITPGIWVACIKAEGVLLDYLVLLPRDYYEASVLQLPVTEPCAYAGPPQENCLLYQHLPVTRFPCTLACEARHFLLDGEPRPVAVRQPTPAHPVMVDLSGREVELHLRLRIPQVGHYVVVVEYSTEAAQLFVVDVNVKSSGSVLAGQVNIYSCNYSVLCRSAVIDHMSRIAMYELLADADIQLKGHMARFLLHQVCIIPIEEFSAEYVRPQVHCIASYGRFVNQSATCVSLAHETPPTALILDVLSGRPFPHLPQQSSPSVDVLPGVTLKAPQNQVTLRGRVPHLGRYVFVIHFYQAAHPTFPAQVSVDGGWPRAGSFHASFCPHVLGCRDQVIAEGQIEFDISEPEVAATVKVPEGKSLVLVRVLVVPAENYDYQILHKKSMDKSLEFITNCGKNSFYLDPQTASRFCKNSARSLVAFYHKGALPCECHPTGATGPHCSPEGGQCPCQPNVIGRQCTRCATGHYGFPRCKPCSCGRRLCEEMTGQCRCPPRTVRPQCEVCETHSFSFHPMAGCEGCNCSRRGTIEAAMPECDRDSGQCRCKPRITGRQCDRCASGFYRFPECVPCNCNRDGTEPGVCDPGTGACLCKENVEGTECNVCREGSFHLDPANLKGCTSCFCFGVNNQCHSSHKRRTKFVDMLGWHLETADRVDIPVSFNPGSNSMVADLQELPATIHSASWVAPTSYLGDKVSSYGGYLTYQAKSFGLPGDMVLLEKKPDVQLTGQHMSIIYEETNTPRPDRLHHGRVHVVEGNFRHASSRAPVSREELMTVLSRLADVRIQGLYFTETQRLTLSEVGLEEASDTGSGRIALAVEICACPPAYAGDSCQGCSPGYYRDHKGLYTGRCVPCNCNGHSNQCQDGSGICVNCQHNTAGEHCERCQEGYYGNAVHGSCRACPCPHTNSFATGCVVNGGDVRCSCKAGYTGTQCERCAPGYFGNPQKFGGSCQPCSCNSNGQLGSCHPLTGDCINQEPKDSSPAEECDDCDSCVMTLLNDLATMGEQLRLVKSQLQGLSASAGLLEQMRHMETQAKDLRNQLLNYRSAISNHGSKIEGLERELTDLNQEFETLQEKAQVNSRKAQTLNNNVNRATQSAKELDVKIKNVIRNVHILLKQISGTDGEGNNVPSGDFSREWAEAQRMMRELRNRNFGKHLREAEADKRESQLLLNRIRTWQKTHQGENNGLANSIRDSLNEYEAKLSDLRARLQEAAAQAKQANGLNQENERALGAIQRQVKEINSLQSDFTKYLTTADSSLLQTNIALQLMEKSQKEYEKLAASLNEARQELSDKVRELSRSAGKTSLVEEAEKHARSLQELAKQLEEIKRNASGDELVRCAVDAATAYENILNAIKAAEDAANRAASASESALQTVIKEDLPRKAKTLSSNSDKLLNEAKMTQKKLKQEVSPALNNLQQTLNIVTVQKEVIDTNLTTLRDGLHGIQRGDIDAMISSAKSMVRKANDITDEVLDGLNPIQTDVERIKDTYGRTQNEDFKKALTDADNSVNKLTNKLPDLWRKIESINQQLLPLGNISDNMDRIRELIQQARDAASKVAVPMRFNGKSGVEVRLPNDLEDLKGYTSLSLFLQRPNSRENGGTENMFVMYLGNKDASRDYIGMAVVDGQLTCVYNLGDREAELQVDQILTKSETKEAVMDRVKFQRIYQFARLNYTKGATSSKPETPGVYDMDGRNSNTLLNLDPENVVFYVGGYPPDFKLPSRLSFPPYKGCIELDDLNENVLSLYNFKKTFNLNTTEVEPCRRRKEESDKNYFEGTGYARVPTQPHAPIPTFGQTIQTTVDRGLLFFAENGDRFISLNIEDGKLMVRYKLNSELPKERGVGDAINNGRDHSIQIKIGKLQKRMWINVDVQNTIIDGEVFDFSTYYLGGIPIAIRERFNISTPAFRGCMKNLKKTSGVVRLNDTVGVTKKCSEDWKLVRSASFSRGGQLSFTDLGLPPTDHLQASFGFQTFQPSGILLDHQTWTRNLQVTLEDGYIELSTSDSGGPIFKSPQTYMDGLLHYVSVISDNSGLRLLIDDQLLRNSKRLKHISSSRQSLRLGGSNFEGCISNVFVQRLSLSPEVLDLTSNSLKRDVSLGGCSLNKPPFLMLLKGSTRFNKTKTFRINQLLQDTPVASPRSVKVWQDACSPLPKTQANHGALQFGDIPTSHLLFKLPQELLKPRSQFAVDMQTTSSRGLVFHTGTKNSFMALYLSKGRLVFALGTDGKKLRIKSKEKCNDGKWHTVVFGHDGEKGRLVVDGLRAREGSLPGNSTISIRAPVYLGSPPSGKPKSLPTNSFVGCLKNFQLDSKPLYTPSSSFGVSSCLGGPLEKGIYFSEEGGHVVLAHSVLLGPEFKLVFSIRPRSLTGILIHIGSQPGKHLCVYLEAGKVTASMDSGAGGTSTSVTPKQSLCDGQWHSVAVTIKQHILHLELDTDSSYTAGQIPFPPASTQEPLHLGGAPANLTTLRIPVWKSFFGCLRNIHVNHIPVPVTEALEVQGPVSLNGCPDQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|                                                    | 7/142246 [01:25<480:18:57, 12.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm(train_proteins\u001b[38;5;241m.\u001b[39mvalues(), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msequence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[1;32mIn[29], line 12\u001b[0m, in \u001b[0;36mgenerate_embedding\u001b[1;34m(sequence)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Generate the embedding\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 12\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mlast_hidden_state[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1974\u001b[0m, in \u001b[0;36mT5EncoderModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1956\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m   1958\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1970\u001b[0m \u001b[38;5;124;03m>>> last_hidden_states = outputs.last_hidden_state\u001b[39;00m\n\u001b[0;32m   1971\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m   1972\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1974\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1975\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1977\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1979\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1980\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1982\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoder_outputs\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1109\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1094\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1095\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[0;32m   1096\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1106\u001b[0m         output_attentions,\n\u001b[0;32m   1107\u001b[0m     )\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1109\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\t5\\modeling_t5.py:749\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    746\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m    748\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\t5\\modeling_t5.py:338\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m    337\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 338\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\t5\\modeling_t5.py:284\u001b[0m, in \u001b[0;36mT5DenseActDense.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m    283\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwi(hidden_states)\n\u001b[1;32m--> 284\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8\n\u001b[0;32m    290\u001b[0m     ):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\modules\\activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\torch\\nn\\functional.py:1473\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for data in tqdm(train_proteins.values(), desc=\"Generating embeddings\"):\n",
    "    print(data['sequence'])\n",
    "    data['embedding'] = generate_embedding(data['sequence']).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d32ed9-2a00-4568-897a-7ab402b966cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_proteins.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60207af-09a8-43e3-ad95-434c9a5fd301",
   "metadata": {},
   "source": [
    "## Integrating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f2bbb2-035b-4d0a-beb4-a2722a73faaa",
   "metadata": {},
   "source": [
    "Now when we have our protein sequences prepared, we can simply integrate the data, which will give us only one table, that contains all protein sequences and their corresponding taxonomy and GO Term IDs with respect to all aspects (MF, BP, and CC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "457e20ff-ca7b-4829-b08b-d225ae919900",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in train_taxonomy.iterrows():\n",
    "    if row['EntryID'] in train_proteins:\n",
    "        embedded_train_proteins[row['EntryID']]['taxonomyID'] = row['taxonomyID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2e0a6028-e16a-416a-92ba-c90584d61ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in train_terms.iterrows():\n",
    "    if row['EntryID'] in train_proteins:\n",
    "        embedded_train_proteins[row['EntryID']]['GO'][row['aspect']].append(row['term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fd557f04-f0cd-46c4-946e-09d876071b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for protein_id, data in embedded_train_proteins.items():\n",
    "    data_list.append([\n",
    "        protein_id,\n",
    "        data['taxonomyID'],\n",
    "        data['sequence'],\n",
    "        data['embedding'],\n",
    "        data['GO']['BPO'],\n",
    "        data['GO']['CCO'],\n",
    "        data['GO']['MFO']\n",
    "    ])\n",
    "\n",
    "    protein_features = [\n",
    "        protein_id,            \n",
    "        data['taxonomyID'],     \n",
    "    ]\n",
    "\n",
    "    protein_features.extend(data['embedding'])\n",
    "    features.append(protein_features)\n",
    "\n",
    "    labels.append([\n",
    "        protein_id,\n",
    "        data['GO']['BPO'],\n",
    "        data['GO']['CCO'],\n",
    "        data['GO']['MFO']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d28446df-093d-4310-b16d-5c2ec09db60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(data_list, columns=['ProteinID', 'TaxonomyID', 'Sequence', 'Embedding', 'BPO', 'CCO', 'MFO'])\n",
    "train_df.set_index('ProteinID', inplace=True)\n",
    "train_df.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "95729eb0-d78e-4c29-ab76-de397e5091b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TaxonomyID</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>BPO</th>\n",
       "      <th>CCO</th>\n",
       "      <th>MFO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P20536</th>\n",
       "      <td>10249</td>\n",
       "      <td>MNSVTVSHAPYTITYHDDWEPVMSQLVEFYNEVASWLLRDETSPIP...</td>\n",
       "      <td>[-0.090443626, -0.1601761, -0.020129055, 0.067...</td>\n",
       "      <td>[GO:0008152, GO:0071897, GO:0044249, GO:000625...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[GO:0005515, GO:0005488, GO:0003674]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O73864</th>\n",
       "      <td>7955</td>\n",
       "      <td>MTEYRNFLLLFITSLSVIYPCTGISWLGLTINGSSVGWNQTHHCKL...</td>\n",
       "      <td>[-0.090443626, -0.1601761, -0.020129055, 0.067...</td>\n",
       "      <td>[GO:0061371, GO:0048589, GO:0051641, GO:004885...</td>\n",
       "      <td>[GO:0071944, GO:0005575, GO:0110165, GO:001602...</td>\n",
       "      <td>[GO:0046982, GO:0003674, GO:0005488, GO:000551...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O95231</th>\n",
       "      <td>9606</td>\n",
       "      <td>MRLSSSPPRGPQQLSSFGSVDWLSQSSCSGPTHTPRPADFSLGSLP...</td>\n",
       "      <td>[-0.090443626, -0.1601761, -0.020129055, 0.067...</td>\n",
       "      <td>[GO:0006357, GO:0010557, GO:0045935, GO:006500...</td>\n",
       "      <td>[GO:0005622, GO:0031981, GO:0043229, GO:004322...</td>\n",
       "      <td>[GO:0003676, GO:1990837, GO:0001216, GO:000548...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TaxonomyID                                           Sequence  \\\n",
       "P20536       10249  MNSVTVSHAPYTITYHDDWEPVMSQLVEFYNEVASWLLRDETSPIP...   \n",
       "O73864        7955  MTEYRNFLLLFITSLSVIYPCTGISWLGLTINGSSVGWNQTHHCKL...   \n",
       "O95231        9606  MRLSSSPPRGPQQLSSFGSVDWLSQSSCSGPTHTPRPADFSLGSLP...   \n",
       "\n",
       "                                                Embedding  \\\n",
       "P20536  [-0.090443626, -0.1601761, -0.020129055, 0.067...   \n",
       "O73864  [-0.090443626, -0.1601761, -0.020129055, 0.067...   \n",
       "O95231  [-0.090443626, -0.1601761, -0.020129055, 0.067...   \n",
       "\n",
       "                                                      BPO  \\\n",
       "P20536  [GO:0008152, GO:0071897, GO:0044249, GO:000625...   \n",
       "O73864  [GO:0061371, GO:0048589, GO:0051641, GO:004885...   \n",
       "O95231  [GO:0006357, GO:0010557, GO:0045935, GO:006500...   \n",
       "\n",
       "                                                      CCO  \\\n",
       "P20536                                                 []   \n",
       "O73864  [GO:0071944, GO:0005575, GO:0110165, GO:001602...   \n",
       "O95231  [GO:0005622, GO:0031981, GO:0043229, GO:004322...   \n",
       "\n",
       "                                                      MFO  \n",
       "P20536               [GO:0005515, GO:0005488, GO:0003674]  \n",
       "O73864  [GO:0046982, GO:0003674, GO:0005488, GO:000551...  \n",
       "O95231  [GO:0003676, GO:1990837, GO:0001216, GO:000548...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6b72e230-8c90-4ce1-a7d7-f6fea42c2a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['ProteinID', 'TaxonomyID'] + ['Embed_' + str(i+1) for i in range(1024)]\n",
    "X_train = pd.DataFrame(features, columns=column_names)\n",
    "X_train.set_index('ProteinID', inplace=True)\n",
    "X_train.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dac6838a-f6f4-4e9b-b74d-a665769ba3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TaxonomyID</th>\n",
       "      <th>Embed_1</th>\n",
       "      <th>Embed_2</th>\n",
       "      <th>Embed_3</th>\n",
       "      <th>Embed_4</th>\n",
       "      <th>Embed_5</th>\n",
       "      <th>Embed_6</th>\n",
       "      <th>Embed_7</th>\n",
       "      <th>Embed_8</th>\n",
       "      <th>Embed_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Embed_1015</th>\n",
       "      <th>Embed_1016</th>\n",
       "      <th>Embed_1017</th>\n",
       "      <th>Embed_1018</th>\n",
       "      <th>Embed_1019</th>\n",
       "      <th>Embed_1020</th>\n",
       "      <th>Embed_1021</th>\n",
       "      <th>Embed_1022</th>\n",
       "      <th>Embed_1023</th>\n",
       "      <th>Embed_1024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P20536</th>\n",
       "      <td>10249</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O73864</th>\n",
       "      <td>7955</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O95231</th>\n",
       "      <td>9606</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0A0B4J1F4</th>\n",
       "      <td>10090</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P54366</th>\n",
       "      <td>7227</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P33681</th>\n",
       "      <td>9606</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P77596</th>\n",
       "      <td>83333</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q16787</th>\n",
       "      <td>9606</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q59VP0</th>\n",
       "      <td>237561</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P13508</th>\n",
       "      <td>6239</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q96S79</th>\n",
       "      <td>9606</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q9VSA3</th>\n",
       "      <td>7227</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O94652</th>\n",
       "      <td>284812</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P35669</th>\n",
       "      <td>284812</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q5MNZ6</th>\n",
       "      <td>9606</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q9ET09</th>\n",
       "      <td>10116</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E7EZG2</th>\n",
       "      <td>7955</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q7ZT11</th>\n",
       "      <td>9031</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q96C12</th>\n",
       "      <td>9606</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P05179</th>\n",
       "      <td>10116</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P41216</th>\n",
       "      <td>10090</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q9VX31</th>\n",
       "      <td>7227</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P04632</th>\n",
       "      <td>9606</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P04021</th>\n",
       "      <td>10254</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q86KP5</th>\n",
       "      <td>44689</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O75908</th>\n",
       "      <td>9606</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P82343</th>\n",
       "      <td>10090</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P85298</th>\n",
       "      <td>9606</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P06846</th>\n",
       "      <td>83333</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O48573</th>\n",
       "      <td>3702</td>\n",
       "      <td>-0.090444</td>\n",
       "      <td>-0.160176</td>\n",
       "      <td>-0.020129</td>\n",
       "      <td>0.067913</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.043394</td>\n",
       "      <td>0.041893</td>\n",
       "      <td>-0.16812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054649</td>\n",
       "      <td>-0.084524</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.015732</td>\n",
       "      <td>-0.013776</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.090861</td>\n",
       "      <td>-0.008414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            TaxonomyID   Embed_1   Embed_2   Embed_3   Embed_4   Embed_5  \\\n",
       "P20536           10249 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "O73864            7955 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "O95231            9606 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "A0A0B4J1F4       10090 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "P54366            7227 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "P33681            9606 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "P77596           83333 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "Q16787            9606 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "Q59VP0          237561 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "P13508            6239 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "Q96S79            9606 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "Q9VSA3            7227 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "O94652          284812 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "P35669          284812 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "Q5MNZ6            9606 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "Q9ET09           10116 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "E7EZG2            7955 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "Q7ZT11            9031 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "Q96C12            9606 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "P05179           10116 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "P41216           10090 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "Q9VX31            7227 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "P04632            9606 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "P04021           10254 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "Q86KP5           44689 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "O75908            9606 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "P82343           10090 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "P85298            9606 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "P06846           83333 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "O48573            3702 -0.090444 -0.160176 -0.020129  0.067913 -0.015551   \n",
       "\n",
       "             Embed_6   Embed_7   Embed_8  Embed_9  ...  Embed_1015  \\\n",
       "P20536      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "O73864      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "O95231      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "A0A0B4J1F4  0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "P54366      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "P33681      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "P77596      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "Q16787      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "Q59VP0      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "P13508      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "Q96S79      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "Q9VSA3      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "O94652      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "P35669      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "Q5MNZ6      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "Q9ET09      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "E7EZG2      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "Q7ZT11      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "Q96C12      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "P05179      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "P41216      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "Q9VX31      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "P04632      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "P04021      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "Q86KP5      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "O75908      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "P82343      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "P85298      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "P06846      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "O48573      0.033842  0.043394  0.041893 -0.16812  ...   -0.054649   \n",
       "\n",
       "            Embed_1016  Embed_1017  Embed_1018  Embed_1019  Embed_1020  \\\n",
       "P20536       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "O73864       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "O95231       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "A0A0B4J1F4   -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "P54366       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "P33681       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "P77596       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "Q16787       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "Q59VP0       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "P13508       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "Q96S79       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "Q9VSA3       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "O94652       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "P35669       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "Q5MNZ6       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "Q9ET09       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "E7EZG2       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "Q7ZT11       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "Q96C12       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "P05179       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "P41216       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "Q9VX31       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "P04632       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "P04021       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "Q86KP5       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "O75908       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "P82343       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "P85298       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "P06846       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "O48573       -0.084524    0.044401    0.080548   -0.015732   -0.013776   \n",
       "\n",
       "            Embed_1021  Embed_1022  Embed_1023  Embed_1024  \n",
       "P20536       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "O73864       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "O95231       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "A0A0B4J1F4   -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "P54366       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "P33681       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "P77596       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "Q16787       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "Q59VP0       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "P13508       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "Q96S79       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "Q9VSA3       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "O94652       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "P35669       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "Q5MNZ6       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "Q9ET09       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "E7EZG2       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "Q7ZT11       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "Q96C12       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "P05179       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "P41216       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "Q9VX31       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "P04632       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "P04021       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "Q86KP5       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "O75908       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "P82343       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "P85298       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "P06846       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "O48573       -0.005043   -0.011905   -0.090861   -0.008414  \n",
       "\n",
       "[30 rows x 1025 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0782fa7-1477-47ed-9e42-da48f0fa97f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(features, columns=['ProteinID', 'BPO', 'CCO', 'MFO'])\n",
    "y_train.set_index('ProteinID', inplace=True)\n",
    "y_train.index.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8d0d0-3673-4fe7-9b07-f018a477c989",
   "metadata": {},
   "source": [
    "## Store "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc06c8-55fa-4f8e-87ba-a8bc4fe375aa",
   "metadata": {},
   "source": [
    "Let's save the embedded data as CSV files for both training and testing so we could easily access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e144416-b635-48c0-badf-464e88fc7ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('data_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f8186-ad96-4390-a289-cee6afd5bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('features_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a823d71-1920-4f30-8687-b3321a806486",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv('labels_train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
